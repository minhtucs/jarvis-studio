{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c64cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To stream OpenAI chat responses to a client using Python, you typically want to use a web framework such as Flask in combination with server-sent events (SSE) or WebSockets to send real-time data from your server to your client. Below, I'll show you a simple example using Flask and SSE to stream OpenAI chat responses.\n",
       "\n",
       "For this example, we'll assume you have the OpenAI package installed, and you have set up your OpenAI API key correctly.\n",
       "\n",
       "### Prerequisites\n",
       "1. Install the required packages:\n",
       "   ```bash\n",
       "   pip install Flask openai\n",
       "   ```\n",
       "\n",
       "### Flask Server Code\n",
       "\n",
       "```python\n",
       "from flask import Flask, Response, request\n",
       "import openai\n",
       "import os\n",
       "\n",
       "app = Flask(__name__)\n",
       "\n",
       "# Set up your OpenAI API key\n",
       "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # You can also hardcode your API key, but it's not recommended\n",
       "\n",
       "def generate_chat_response(prompt):\n",
       "    # This function will generate responses from OpenAI's chat model\n",
       "    response = openai.ChatCompletion.create(\n",
       "        model=\"gpt-3.5-turbo\",  # Specify the model you want to use\n",
       "        messages=[\n",
       "            {\"role\": \"user\", \"content\": prompt}\n",
       "        ],\n",
       "        stream=True  # Enable streaming\n",
       "    )\n",
       "    \n",
       "    return response\n",
       "\n",
       "@app.route('/stream_chat', methods=['GET'])\n",
       "def stream_chat():\n",
       "    prompt = request.args.get('prompt')\n",
       "    \n",
       "    def generate():\n",
       "        if prompt:\n",
       "            for chunk in generate_chat_response(prompt):\n",
       "                # Each chunk is a streamed part of response\n",
       "                if 'choices' in chunk and len(chunk['choices']) > 0:\n",
       "                    data = chunk['choices'][0].get('delta', {}).get('content', '')\n",
       "                    if data:\n",
       "                        yield f\"data: {data}\\n\\n\"\n",
       "    \n",
       "    return Response(generate(), content_type='text/event-stream')\n",
       "\n",
       "if __name__ == '__main__':\n",
       "    app.run(debug=True, threaded=True)\n",
       "```\n",
       "\n",
       "### Client-Side Code\n",
       "\n",
       "You can use HTML and JavaScript to consume the SSE and display the response on the client side.\n",
       "\n",
       "```html\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <title>OpenAI Streaming Chat</title>\n",
       "</head>\n",
       "<body>\n",
       "    <h1>OpenAI Chat Streaming</h1>\n",
       "    <input type=\"text\" id=\"prompt\" placeholder=\"Enter your prompt\">\n",
       "    <button id=\"sendButton\">Send</button>\n",
       "    <div id=\"responseContainer\"></div>\n",
       "\n",
       "    <script>\n",
       "        document.getElementById(\"sendButton\").onclick = function() {\n",
       "            const prompt = document.getElementById(\"prompt\").value;\n",
       "            const responseContainer = document.getElementById(\"responseContainer\");\n",
       "            responseContainer.innerHTML = \"\"; // Clear previous responses\n",
       "\n",
       "            const eventSource = new EventSource(`/stream_chat?prompt=${encodeURIComponent(prompt)}`);\n",
       "\n",
       "            eventSource.onmessage = function(event) {\n",
       "                responseContainer.innerHTML += event.data; // Append new data to the response container\n",
       "            };\n",
       "\n",
       "            eventSource.onerror = function(event) {\n",
       "                console.error(\"Error occurred\", event);\n",
       "                eventSource.close(); // Close the EventSource on error\n",
       "            };\n",
       "        };\n",
       "    </script>\n",
       "</body>\n",
       "</html>\n",
       "```\n",
       "\n",
       "### Running the Application\n",
       "1. Set your OpenAI API key in your environment:\n",
       "   ```bash\n",
       "   export OPENAI_API_KEY='your-api-key-here'\n",
       "   ```\n",
       "2. Run the Flask application:\n",
       "   ```bash\n",
       "   python your_flask_file.py\n",
       "   ```\n",
       "3. Open the created HTML file in a browser and input a prompt to see the streamed response.\n",
       "\n",
       "### Important Notes\n",
       "- Replace `model=\"gpt-3.5-turbo\"` with the appropriate model as per your requirement.\n",
       "- Be mindful of API usage and costs when using the OpenAI API.\n",
       "- This example uses `stream=True` which enables real-time output from OpenAI's chat model. Make sure you handle streams properly in your application."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "content = \"To stream OpenAI chat responses to a client using Python, you typically want to use a web framework such as Flask in combination with server-sent events (SSE) or WebSockets to send real-time data from your server to your client. Below, I'll show you a simple example using Flask and SSE to stream OpenAI chat responses.\\n\\nFor this example, we'll assume you have the OpenAI package installed, and you have set up your OpenAI API key correctly.\\n\\n### Prerequisites\\n1. Install the required packages:\\n   ```bash\\n   pip install Flask openai\\n   ```\\n\\n### Flask Server Code\\n\\n```python\\nfrom flask import Flask, Response, request\\nimport openai\\nimport os\\n\\napp = Flask(__name__)\\n\\n# Set up your OpenAI API key\\nopenai.api_key = os.getenv(\\\"OPENAI_API_KEY\\\")  # You can also hardcode your API key, but it's not recommended\\n\\ndef generate_chat_response(prompt):\\n    # This function will generate responses from OpenAI's chat model\\n    response = openai.ChatCompletion.create(\\n        model=\\\"gpt-3.5-turbo\\\",  # Specify the model you want to use\\n        messages=[\\n            {\\\"role\\\": \\\"user\\\", \\\"content\\\": prompt}\\n        ],\\n        stream=True  # Enable streaming\\n    )\\n    \\n    return response\\n\\n@app.route('/stream_chat', methods=['GET'])\\ndef stream_chat():\\n    prompt = request.args.get('prompt')\\n    \\n    def generate():\\n        if prompt:\\n            for chunk in generate_chat_response(prompt):\\n                # Each chunk is a streamed part of response\\n                if 'choices' in chunk and len(chunk['choices']) > 0:\\n                    data = chunk['choices'][0].get('delta', {}).get('content', '')\\n                    if data:\\n                        yield f\\\"data: {data}\\\\n\\\\n\\\"\\n    \\n    return Response(generate(), content_type='text/event-stream')\\n\\nif __name__ == '__main__':\\n    app.run(debug=True, threaded=True)\\n```\\n\\n### Client-Side Code\\n\\nYou can use HTML and JavaScript to consume the SSE and display the response on the client side.\\n\\n```html\\n<!DOCTYPE html>\\n<html lang=\\\"en\\\">\\n<head>\\n    <meta charset=\\\"UTF-8\\\">\\n    <meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1.0\\\">\\n    <title>OpenAI Streaming Chat</title>\\n</head>\\n<body>\\n    <h1>OpenAI Chat Streaming</h1>\\n    <input type=\\\"text\\\" id=\\\"prompt\\\" placeholder=\\\"Enter your prompt\\\">\\n    <button id=\\\"sendButton\\\">Send</button>\\n    <div id=\\\"responseContainer\\\"></div>\\n\\n    <script>\\n        document.getElementById(\\\"sendButton\\\").onclick = function() {\\n            const prompt = document.getElementById(\\\"prompt\\\").value;\\n            const responseContainer = document.getElementById(\\\"responseContainer\\\");\\n            responseContainer.innerHTML = \\\"\\\"; // Clear previous responses\\n\\n            const eventSource = new EventSource(`/stream_chat?prompt=${encodeURIComponent(prompt)}`);\\n\\n            eventSource.onmessage = function(event) {\\n                responseContainer.innerHTML += event.data; // Append new data to the response container\\n            };\\n\\n            eventSource.onerror = function(event) {\\n                console.error(\\\"Error occurred\\\", event);\\n                eventSource.close(); // Close the EventSource on error\\n            };\\n        };\\n    </script>\\n</body>\\n</html>\\n```\\n\\n### Running the Application\\n1. Set your OpenAI API key in your environment:\\n   ```bash\\n   export OPENAI_API_KEY='your-api-key-here'\\n   ```\\n2. Run the Flask application:\\n   ```bash\\n   python your_flask_file.py\\n   ```\\n3. Open the created HTML file in a browser and input a prompt to see the streamed response.\\n\\n### Important Notes\\n- Replace `model=\\\"gpt-3.5-turbo\\\"` with the appropriate model as per your requirement.\\n- Be mindful of API usage and costs when using the OpenAI API.\\n- This example uses `stream=True` which enables real-time output from OpenAI's chat model. Make sure you handle streams properly in your application.\"\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
